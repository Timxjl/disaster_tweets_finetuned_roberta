# Disaster Tweets Fine-Tuned RoBERTa

Welcome to the Disaster Tweets Fine-Tuned RoBERTa repository! This repository contains a natural language processing (NLP) model that was fine-tuned for the Kaggle competition "Natural Language Processing with Disaster Tweets." You can find the competition leaderboard [here](https://www.kaggle.com/competitions/nlp-getting-started/leaderboard).

## Overview

- Model Architecture: RoBERTa-base
- F1-score: 0.83420
- Leaderboard Ranking: 133rd
- Kaggle Username: Tim Xiong

## Model Details

The model in this repository is based on the RoBERTa architecture, which is a variant of the BERT model, pre-trained by Hugging Face. RoBERTa is known for its exceptional performance on various NLP tasks, including text classification, which makes it a great choice for the disaster tweet classification task.

## Usage

To use this fine-tuned RoBERTa model for your own text classification tasks, you can follow these steps:

1. Clone this repository to your local machine:

```bash
git clone https://github.com/Timxjl/disaster_tweets_finetuned_roberta.git
